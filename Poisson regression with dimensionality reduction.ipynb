{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHOOSING IMPORTANT FEATURES\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from itertools import product\n",
    "from sklearn.metrics import r2_score\n",
    "plt.rcParams['figure.figsize'] = (16.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.describe_option('display')\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.read_csv('full.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.drop(full.columns[full.columns.str.contains('unnamed',case = False)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['WEIGHT'] = full['WEIGHT'].replace(1.000000e+16,np.nan)\n",
    "full['WEIGHT'] = full['WEIGHT'].replace(' ',np.nan)\n",
    "full['WEIGHT'] = full['WEIGHT'].fillna(9)\n",
    "full['WEIGHT'] = full['WEIGHT'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### deleting complete record afor a missing value\n",
    "full = full.dropna()\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.columns #no last_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_c = pd.DataFrame(data = full, columns =['ACC_TYPE','BUS_USE','CDL_STAT','DAY','DRIMPAIR','DR_DRINK','DR_HGT','DR_SF1','DR_SF2','DR_SF3','DR_SF4',\n",
    "          'EMER_USE','HARM_EV','HOUR','HAZ_PLAC','IMPACT1','LAST_YR','L_COMPL','L_RESTRI','L_STATUS','L_TYPE','MAKE','MAN_COLL',\n",
    "          'MDRDSTRD','MDRMANAV','MFACTOR','MINUTE','MOD_YEAR','MONTH','MVIOLATN','MVISOBSC','OWNER','P_CRASH2','ROLLOVER','SPEC_USE',\n",
    "          'SPEEDREL','STATE','TOW_VEH','VALIGN','VEH_SC1','VEH_SC2','VINTYPE','VPAVETYP','VPROFILE','VSURCOND','VTCONT_F','VTRAFCON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_c = full_c.apply(lambda x: x.astype('category')) #categories\n",
    "#print(full_c.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_o = pd.DataFrame(data = full, columns =['DR_WGT','GVWR','WEIGHT'])#.apply(lambda x: x.astype('category'), ordered = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_o = full_o.apply(lambda x: x.astype('category',ordered=True)) #Ordinal\n",
    "#print(full_o.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_p = pd.DataFrame(data = full, columns =['DEATHS','NUMOCCS','PREV_ACC','PREV_DWI','PREV_SPD','PREV_SUS','VE_FORMS','VNUM_LAN',\n",
    "                                             'VTRAFWAY','FIRE_EXP','HAZ_INV','HIT_RUN','TRAV_SP'])\n",
    "# count variables were left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([full_o, full_c, full_p], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vin_type = full.copy()\n",
    "vin_type.head()\n",
    "\n",
    "clean_up = {\"VINTYPE\" : {'P' : 0, \"T\" : 1, \"M\" : 2,\"U\" : 3,\"C\" : 4}}\n",
    "\n",
    "vin_type.replace(clean_up, inplace = True)\n",
    "print(vin_type.head())\n",
    "\n",
    "full['VINTYPE'] = vin_type['VINTYPE']\n",
    "full['VINTYPE'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape #(296847, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = full.columns.tolist()\n",
    "cols\n",
    "cols = ['DR_WGT','GVWR','WEIGHT', 'ACC_TYPE', 'BUS_USE', 'CDL_STAT', 'DAY', 'DRIMPAIR', 'DR_DRINK', 'DR_HGT', 'DR_SF1', 'DR_SF2', 'DR_SF3',\n",
    "        'DR_SF4','EMER_USE','HARM_EV','HOUR','HAZ_PLAC','IMPACT1','LAST_YR', 'L_COMPL', 'L_RESTRI', 'L_STATUS', 'L_TYPE', 'MAKE',\n",
    "        'MAN_COLL','MDRDSTRD', 'MDRMANAV', 'MFACTOR', 'MINUTE', 'MOD_YEAR', 'MONTH', 'MVIOLATN', 'MVISOBSC', 'OWNER', 'P_CRASH2',\n",
    "        'ROLLOVER','SPEC_USE', 'SPEEDREL', 'STATE', 'TOW_VEH', 'VALIGN', 'VEH_SC1', 'VEH_SC2', 'VINTYPE', 'VPAVETYP', 'VPROFILE',\n",
    "        'VSURCOND','VTCONT_F', 'VTRAFCON', 'PREV_DWI', 'NUMOCCS', 'PREV_ACC', 'PREV_SPD', 'PREV_SUS', 'VE_FORMS', 'VNUM_LAN',\n",
    "        'VTRAFWAY', 'FIRE_EXP', 'HAZ_INV', 'HIT_RUN', 'TRAV_SP', 'DEATHS']\n",
    "full = full[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A NEW BINARY VARIABLE : 0 -if no deaths. 1- if # of deaths >= 1\n",
    "full['DEATHS_BIN'] = full['DEATHS'].apply(lambda x: 1 if x>=1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"NUMBER OF DEATHS\")\n",
    "print(full['DEATHS'].unique())\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.hist(full['DEATHS']) #99 - unknown\n",
    "plt.ylabel('NUMBER OF DEATHS')\n",
    "plt.show()\n",
    "print(\"Mean=\",np.mean(full['DEATHS']))\n",
    "print(\"Variance=\",np.std(full['DEATHS'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full.to_csv('./DataStorage/NHTSA/MISCELLANOUS/full_latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full = pd.read_csv('./DataStorage/NHTSA/MISCELLANOUS/full_latest.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full = full.drop(full.columns[full.columns.str.contains('unnamed',case = False)],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 : CORRELATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data, to):\n",
    "    converted = None\n",
    "    if to == 'array':\n",
    "        if isinstance(data, np.ndarray):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values\n",
    "        elif isinstance(data, list):\n",
    "            converted = np.array(data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            converted = data.as_matrix()\n",
    "    elif to == 'list':\n",
    "        if isinstance(data, list):\n",
    "            converted = data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = data.tolist()\n",
    "    elif to == 'dataframe':\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            converted = data\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = pd.DataFrame(data)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data conversion: {}\".format(to))\n",
    "    if converted is None:\n",
    "        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n",
    "    else:\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CORRELATION BETWEEN ALL THE VARIABLES\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def function(full, nominal_columns=None, **kwargs):\n",
    "    columns = full.columns\n",
    "    corr = pd.DataFrame(index=columns, columns=columns) # converts it like a matrix\n",
    "    if nominal_columns is None:\n",
    "        nominal_columns = list()\n",
    "    elif nominal_columns == 'all':\n",
    "        nominal_columns = columns\n",
    "    for i in range(0,len(columns)):\n",
    "            for j in range(i,len(columns)):\n",
    "                if i == j:\n",
    "                    corr[columns[i]][columns[j]] = 1.0\n",
    "                else:\n",
    "                    if columns[i] in nominal_columns:\n",
    "                        if columns[j] in nominal_columns:    #BOTH NOMINAL\n",
    "                            cell = cramers_v(full[columns[i]],full[columns[j]])\n",
    "                            #result=\"{} and {} are IMPORTANT = {}\".format((i), (j),(cell)) \n",
    "                            #print(result)\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                        else:                               #1ST - NOMINAL, 2ND - CONTINUOUS\n",
    "                            cell = correlation_ratio(full[columns[i]], full[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                    else:                                   #1ST - CONTINUOUS, 2ND - NOMINAL\n",
    "                        if columns[j] in nominal_columns:\n",
    "                            cell = correlation_ratio(full[columns[j]], full[columns[i]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell\n",
    "                        else:                               #BOTH CONTINUOUS\n",
    "                            cell, _ = ss.pearsonr(full[columns[i]], full[columns[j]])\n",
    "                            corr[columns[i]][columns[j]] = cell\n",
    "                            corr[columns[j]][columns[i]] = cell                    \n",
    "    #print(corr)\n",
    "    writer = pd.ExcelWriter('output1.xlsx')\n",
    "    corr.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "    return corr\n",
    "\n",
    "def correlation_ratio(categories, measurements): \n",
    "    categories = convert(categories, 'array')\n",
    "    measurements = convert(measurements, 'array')\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = numerator/denominator\n",
    "    return eta    \n",
    "    \n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "    \n",
    "nominal_columns = ['ACC_TYPE','BUS_USE','CDL_STAT','DAY','DRIMPAIR','DR_DRINK','DR_HGT','DR_SF1','DR_SF2','DR_SF3','DR_SF4',\n",
    "          'EMER_USE','HARM_EV','HOUR','HAZ_PLAC','IMPACT1','LAST_YR','L_COMPL','L_RESTRI','L_STATUS','L_TYPE','MAKE','MAN_COLL',\n",
    "          'MDRDSTRD','MDRMANAV','MFACTOR','MINUTE','MOD_YEAR','MONTH','MVIOLATN','MVISOBSC','OWNER','P_CRASH2','ROLLOVER','SPEC_USE',\n",
    "          'SPEEDREL','STATE','TOW_VEH','VALIGN','VEH_SC1','VEH_SC2','VINTYPE','VPAVETYP','VPROFILE','VSURCOND','VTCONT_F','VTRAFCON',\n",
    "           'DR_WGT','GVWR','WEIGHT']\n",
    "corr = function(full,nominal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBSERVATIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GVWR          MAKE      0.429  #REMOVE MAKE\n",
    "WEIGHT        MAKE      0.431\n",
    "WEIGHT        VIN_TYPE  0.498\n",
    "P_CRASH2      ACC_TYPE  0.434\n",
    "L_TYPE        L_COMPL   0.445  #REMOVE L_TYPE\n",
    "L_TYPE        L_STATUS  0.464\n",
    "P_CRASH2      MAN_COLL  0.435  #REMOVE MAN_COLL\n",
    "STATE         TRAV_SP   0.455\n",
    "VALIGN        VPAVETYP  0.421  #REMOVE VALIGN\n",
    "VALIGN        VPROFILE  0.499\n",
    "VALIGN        VSURCOND  0.420\n",
    "VEH_SC1       VEH_SC2   0.450  #REMOVE VEH_SC2 \n",
    "\n",
    "WEIGHT         GVWR     0.563\n",
    "ACC_TYPE      MAN_COLL  0.576\n",
    "L_STATUS      L_COMPL   0.558\n",
    "VIN_TYPE      MAKE      0.593\n",
    "\n",
    "VTRAFCON      VTCONT_F  0.635\n",
    "DR_DRINK      DRIMPAIR  0.683  ### DRIMPAIR\n",
    "HAZ_PLAC      HAZ_INV   1      #REMOVE HAZ_PLAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing IRRELEVANT variables from the data\n",
    "full = full.drop(['MAKE','L_TYPE','MAN_COLL','VALIGN','VEH_SC2','HAZ_PLAC','DRIMPAIR'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataset for other test\n",
    "full_data = full.copy() # full_data is actual dataset after correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing COUNT variables from the data\n",
    "full = full.drop(['VTRAFWAY','NUMOCCS','VNUM_LAN','VE_FORMS','PREV_ACC','PREV_DWI','PREV_SUS','PREV_SPD','DEATHS'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://www.insightsbot.com/blog/2AeuRL/chi-square-feature-selection-in-python\n",
    "Rules to use the Chi-Square Test:\n",
    "\n",
    "1. Variables are Categorical\n",
    "2. Frequency is at least 5\n",
    "3. Variables are sampled independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 : CHISQ TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ChiSquare:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.p = None #P-Value\n",
    "        self.chi2 = None #Chi Test Statistic\n",
    "        self.dof = None\n",
    "        \n",
    "        self.dfObserved = None\n",
    "        self.dfExpected = None\n",
    "        \n",
    "    def TestIndependence(self,colX,colY, alpha=0.05):\n",
    "        X = self.df[colX].astype(str)\n",
    "        Y = self.df[colY].astype(str)\n",
    "        \n",
    "        self.dfObserved = pd.crosstab(Y,X)  #computes the frequency table of the factors\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)  #build contingency table\n",
    "        self.p = p #'%.2E' % Decimal(p) #np.round(p,5)\n",
    "        self.chi2 = chi2\n",
    "        self.dof = dof \n",
    "        \n",
    "        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n",
    "        \n",
    "#Initialize ChiSquare Class\n",
    "cT = ChiSquare(full_data)\n",
    "#Feature Selection\n",
    "testColumns = ['ACC_TYPE', 'BUS_USE', 'CDL_STAT', 'DAY','DR_DRINK', 'DR_HGT', 'DR_SF1', 'DR_SF2', \n",
    "               'DR_SF3', 'DR_SF4', 'DR_WGT','EMER_USE', 'FIRE_EXP', 'GVWR', 'HARM_EV', 'HAZ_INV','HIT_RUN', 'HOUR', \n",
    "               'IMPACT1', 'LAST_YR', 'L_COMPL', 'L_RESTRI','L_STATUS', 'MDRDSTRD', 'MDRMANAV','MFACTOR', 'MINUTE',\n",
    "               'MOD_YEAR', 'MONTH', 'MVIOLATN', 'MVISOBSC','OWNER','P_CRASH2', 'ROLLOVER', 'SPEC_USE', 'SPEEDREL', 'STATE', \n",
    "               'TOW_VEH','TRAV_SP', 'VEH_SC1', 'VINTYPE','VPAVETYP', 'VPROFILE', 'VSURCOND', 'VTCONT_F', 'VTRAFCON', 'WEIGHT']\n",
    "for var in testColumns:\n",
    "    alpha = 0.05\n",
    "    cT.TestIndependence(colX=var,colY='DEATHS')\n",
    "    result = \"\"\n",
    "    if float(cT.p)<alpha:\n",
    "        result=\"{0} is an important predictor.\".format(var)\n",
    "        print(result)\n",
    "    else:   \n",
    "        unimportant = []\n",
    "        x = \"{}-{}\".format((var),(col))\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the variable are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.shape #After removing 6 variables from Correlation algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1 = full_data.copy()\n",
    "#Use this dataframe for next part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 : FEATURE SELECTION USING LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1.head(5) # DEATHS is the target variable(last one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def trainTestSplit(data):\n",
    "    X=np.array(data[list(data.columns[:-2])])\n",
    "    y = full_1['DEATHS']\n",
    "    X.shape\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y, random_state=0)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    return(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = trainTestSplit(full_1)\n",
    "print(\"Shape of the dataset \",np.shape(X_train))\n",
    "print(\"Size of Data set before feature selection: %.2f MB\"%(X_train.nbytes/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross Validation for selecting alpha value\n",
    "feat = full_1.keys().get_values() #Extract attribute names from the data frame\n",
    "lassocv = LassoCV(alphas=None, cv=10, max_iter=100, normalize=True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "lasso = Lasso()\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "print(\"Alpha=\", lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"mse = \",mean_squared_error(y_test, lasso.predict(X_test)))\n",
    "print(\"best model coefficients:\")\n",
    "lasso_coef = pd.Series(np.round(lasso.coef_,4), index=feat[0:-2])\n",
    "lasso_coef.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features selected from Lasso\n",
    "full_2 = lasso_coef[(np.absolute(lasso_coef)>0.0000)]\n",
    "len(full_2) #48\n",
    "full_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4 : FEATURE SELECTION WITH ENET"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#The Elastic Net addresses the aforementioned “over-regularization” by balancing between LASSO and ridge penalties. \n",
    "#In particular, a hyper-parameter, namely Alpha, would be used to regularize the model such that the model would become a\n",
    "#LASSO in case of Alpha = 1 and a ridge in case of Alpha = 0. In practice, Alpha can be tuned easily by the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/cast42/feature-selection-and-elastic-net\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"mean_squared_error\", cv = 10))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratios = [1.5, 1.1, 1, 0.9, 0.8, 0.7, 0.5]\n",
    "alphas = [1e-4, 1e-3, 1e-2, 0.05, 1e-1, 1]\n",
    "cv_elastic = [rmse_cv(ElasticNet(alpha = alpha, l1_ratio=l1_ratio)).mean() for (alpha, l1_ratio) in product(alphas, l1_ratios)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#lt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "idx = list(product(alphas, l1_ratios))\n",
    "p_cv_elastic = pd.Series(cv_elastic, index = idx)\n",
    "p_cv_elastic.plot(title = \"Validation - Just Do It\")\n",
    "plt.xlabel(\"alpha - l1_ratio\")\n",
    "plt.ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic = ElasticNet(alpha=0.0001, l1_ratio=0.7) #(0.0001, 0.5)    0.462328\n",
    "elastic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame({\"preds\":elastic.predict(X_train), \"true\":y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((preds['true']-preds['preds'])**2))\n",
    "print ('RMSE: {0:.4f}'.format(rmse))\n",
    "print('R^2 train: %.3f' %  r2_score(preds['true'], preds['preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(np.round(elastic.coef_,4), index = feat[0:-2])\n",
    "print(\"Elastic Net picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef.to_frame()\n",
    "# Features selected from Lasso\n",
    "full_3 = coef[(np.absolute(coef)>0.0000)]\n",
    "len(full_3) #48\n",
    "full_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "imp_coef = pd.DataFrame(coef.sort_values())\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Elastic Net Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1 = full_1.drop(['DR_WGT','MINUTE','DAY','DR_HGT','MOD_YEAR','LAST_YR','TRAV_SP'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A statistically significant result may not be practically significant.\n",
    "\n",
    "Takeaway: Low p-values don’t necessarily identify predictor variables that are practically important."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The main idea of decision trees is to find those descriptive features which contain the most \"information\" regarding the target\n",
    "feature and then split the dataset along the values of these features such that the target feature values for the resulting \n",
    "sub_datasets are as pure as possible \n",
    "\n",
    "The information gain is a measure of how good a descriptive feature is suited to split a dataset on. To be able to calculate \n",
    "the information gain, we have to first introduce the term entropy of a dataset. The entropy of a dataset is used to measure the \n",
    "impurity of a dataset and we will use this kind of informativeness measure in our calculations. There are also other types of \n",
    "measures which can be used to calculate the information gain. The most prominent ones are the: Gini Index, Chi-Square, \n",
    "Information gain ratio, Variance. \n",
    "\n",
    "The impurity increased, the purity decreased, hence also the entropy increased. Hence we can say, the more \"impure\" a dataset, \n",
    "the higher the entropy and the less \"impure\" a dataset, the lower the entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5 : FEATURE SIGNIFICANCE USING XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = trainTestSplit(full_1) #48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDING ALL THE VARIABLES\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "xg.plot_importance(model)\n",
    "plt.rcParams['figure.figsize'] = (30.0, 26.0)\n",
    "plt.yticks(range(len(full_1.columns)), full_1.columns);\n",
    "plt.show()\n",
    "# TOP 5 ARE PREV_ACC, NUMOCCS, PREV_DWI, VTRAFCON, VTCONT_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_1 = full_1.drop(['VTRAFWAY','NUMOCCS','VNUM_LAN','VE_FORMS','PREV_ACC','PREV_DWI','PREV_SUS','PREV_SPD'], axis = 1)\n",
    "full_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = trainTestSplit(full_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCLUDING NON COUNT VARIABLES\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.plot_importance(model)\n",
    "plt.rcParams['figure.figsize'] = (30.0, 26.0)\n",
    "plt.yticks(range(len(full_1.columns)), full_1.columns);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 6 : Poisson Regression model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Choosing only the top 5 variables from the non count feature-important set + all count variables + DEATH as target\n",
    "#https://www.theanalysisfactor.com/count-data-considered-continuous/\n",
    "var_list = ['VSURCOND','VTRAFCON','VTCONT_F','HIT_RUN','FIRE_EXP','VTRAFWAY','NUMOCCS','VNUM_LAN','VE_FORMS','PREV_ACC','PREV_DWI','PREV_SUS','PREV_SPD','DEATHS']\n",
    "new_df = full_data.loc[:,var_list]\n",
    "new_df.shape\n",
    "#now train - test- validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['DEATHS'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "formula = \"\"\"DEATHS ~ VSURCOND+VTRAFCON+VTCONT_F+HIT_RUN+FIRE_EXP+VTRAFWAY+NUMOCCS+VNUM_LAN+VE_FORMS+PREV_ACC+\n",
    "            PREV_DWI+PREV_SUS+PREV_SPD\"\"\"\n",
    "response, predictors = dmatrices(formula, new_df, return_type='dataframe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_results = sm.GLM(response, predictors, family=sm.families.Poisson()).fit()\n",
    "print(po_results.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Pearson statistic provides an estimate of the data’s dispersion. When the data is drawn from a Poisson distribution with \n",
    "sufficient samples the ratio Pearson chi2 / Df Residuals is approximately 1; for observed data a ratio less than 1 implies \n",
    "underdispersion and more than 1 implies overdispersion. Data that is underdispersed requires a zero-inflated model, which is \n",
    "not directly implemented by Statsmodels. \n",
    "In this case the result, 1795.6, suggests overdispersion, which is amenable to Negative Binomial regression. We explore that \n",
    "below using Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_response(row):\n",
    "    \"Calculate response observation for Cameron-Trivedi dispersion test\"\n",
    "    y = row['DEATHS']\n",
    "    m = row['bev_mu']\n",
    "    return ((y - m)**2 - y) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "ct_data = new_df.copy()\n",
    "ct_data['bev_mu'] = po_results.mu\n",
    "ct_data['ct_resp'] = ct_data.apply(ct_response, axis=1)\n",
    "# Linear regression of auxiliary formula\n",
    "ct_results = smf.ols('ct_resp ~ bev_mu - 1', ct_data).fit()\n",
    "# Construct confidence interval for alpha, the coefficient of bev_mu\n",
    "alpha_ci95 = ct_results.conf_int(0.05).loc['bev_mu']\n",
    "print('\\nC-T dispersion test: alpha = {:5.3f}, 95% CI = ({:5.3f}, {:5.3f})'\n",
    "        .format(ct_results.params[0], alpha_ci95.loc[0], alpha_ci95.loc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_results = sm.GLM(response, predictors,family=sm.families.NegativeBinomial\n",
    "                    (alpha=0.671)).fit()\n",
    "print(nb_results.summary())\n",
    "#https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model does not show promising results. It is overly dispersed model. We decide to convert the target variable to binary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
